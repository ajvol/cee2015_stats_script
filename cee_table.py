#!/usr/bin/python
# -*- coding: utf-8 -*-
# https://meta.wikimedia.org/w/index.php?title=Wikimedia_CEE_Spring_2015

import traceback
import urllib2
import json
import re
import sys
import codecs
from time import gmtime, strftime
from datetime import datetime
import requests
from collections import defaultdict


def savePage(title, content):
    user = 'Botik'
    passw = urllib2.quote(getPasswd())
    baseurl = 'https://meta.wikimedia.org/w/'
    summary = 'auto update'

    # Login request
    payload = {'action': 'query', 'format': 'json', 'utf8': '', 'meta': 'tokens', 'type': 'login'}
    r1 = requests.post(baseurl + 'api.php', data=payload)

    # login confirm
    login_token = r1.json()['query']['tokens']['logintoken']
    payload = {'action': 'login', 'format': 'json', 'utf8': '', 'lgname': user, 'lgpassword': passw, 'lgtoken': login_token}
    r2 = requests.post(baseurl + 'api.php', data=payload, cookies=r1.cookies)

    # get edit token
    payload = {'action': 'query', 'meta': 'tokens', 'format': 'json', 'continue': ''}
    r3 = requests.post(baseurl + 'api.php', data=payload, cookies=r2.cookies)

    edit_token = r3.json()['query']['tokens']['csrftoken']
    edit_cookie = r2.cookies.copy()
    edit_cookie.update(r3.cookies)

    # save action
    headers = {'content-type': 'application/x-www-form-urlencoded'}
    payload = {'format': 'json', 'assert': 'user', 'action': 'edit', 'title': title, 'summary': summary,
               'text': content, 'token': edit_token}
    r4 = requests.post(baseurl + 'api.php', data=payload, headers=headers, cookies=edit_cookie)

    print(r4.text)

def getPasswd():
    try:
        f = open('passwd.txt', 'r')
        p = f.read().rstrip()
        f.close()
        return p
    except:
        print('passwd load error')

def getFirstEdit(server, title):
    timestamp = ''
    if server + ':' + title in cache:
        timestamp = cache[server + ':' + title]
    else:
        co_list_req = urllib2.Request("http://" + server + ".wikipedia.org/w/api.php?format=json&action=query&prop=revisions&titles=" + urllib2.quote(
            title.encode('utf-8')) + "&rvlimit=1&rvprop=timestamp&rvdir=newer&continue=")
        co_list_resp = urllib2.build_opener().open(co_list_req).read()
        co_list_json = json.loads(co_list_resp)

        for itm in co_list_json["query"]["pages"]:
            timestamp = co_list_json["query"]["pages"][itm]["revisions"][0]["timestamp"]
        cache[server + ':' + title] = timestamp
    if timestamp == '':
        timestamp = '2000-01-01T00:00:01Z'

    return datetime.strptime(timestamp, "%Y-%m-%dT%H:%M:%SZ")

def getLastEdit(server, title):
    timestamp = ''

    co_list_req = urllib2.Request("http://" + server + ".wikipedia.org/w/api.php?format=json&action=query&prop=revisions&titles=" + urllib2.quote(
        title.encode('utf-8')) + "&rvlimit=1&rvprop=timestamp&rvdir=older&continue=")
    co_list_resp = urllib2.build_opener().open(co_list_req).read()
    co_list_json = json.loads(co_list_resp)

    for itm in co_list_json["query"]["pages"]:
        timestamp = co_list_json["query"]["pages"][itm]["revisions"][0]["timestamp"]

    if timestamp == '':
        timestamp = '2000-01-01T00:00:01Z'

    return datetime.strptime(timestamp, "%Y-%m-%dT%H:%M:%SZ")


def ResolveRedirects(server, title):
    # http://lt.wikipedia.org/w/api.php?action=query&titles=Riga&redirects

    try:
        if 'r:' + server + ':' + title in cache:
            return cache['r:' + server + ':' + title]

        co_list_req = urllib2.Request("http://" + server + ".wikipedia.org/w/api.php?format=json&action=query&titles=" + urllib2.quote(
            title.encode('utf-8')) + "&redirects&continue=")
        co_list_resp = urllib2.build_opener().open(co_list_req).read()
        co_list_json = json.loads(co_list_resp)

        if 'redirects' in co_list_json["query"]:
            cache['r:' + server + ':' + title] = co_list_json["query"]["redirects"][0]["to"]
            return co_list_json["query"]["redirects"][0]["to"]
        else:
            cache['r:' + server + ':' + title] = title
            return title
    except Exception as e:
        print("".join(traceback.format_exception(sys.exc_info()[0], sys.exc_info()[1], sys.exc_info()[2])))


def PublishStats():
    max_line = 80.0
    t = u''
    t = t + u'{{Notice|This is bot generated statistics about new articles from [[Wikimedia CEE Spring 2016/Article Lists|proposal lists]]. It\'s updated every 2 hours. ' \
            u'Other statistics based on contest template approach, you can find at [[User:BaseBot/CEES/MMXVI]].}}' + '\n'
    t = t + u'{{Notice|Auto generated by [[user:Botik]] at ' + strftime("%Y-%m-%d %H:%M:%S", gmtime()) + ' UTC}}' + '\n'

    t += u":''What could be more stupid than assessing contribution to Wikipedia by counting number of created articles? Only automatic calculation of these figures.''\n"



    t += u'=== New articles total ===\n'
    t += u'{|\n'
    total = sum(stats_by_country.values())
    total2015 = 2838
    max_val = max([total, total2015])
    point = 1.0 * max_line / max_val
    rep = int(round(point * total2015))
    t += u'|-\n'
    t += u'| 2015 CEE Spring articles - || ' + str(total2015) + ' || ' + (u'▒' * rep) + '\n'
    rep = int(round(point * total))
    t += u'|-\n'
    t += u'| 2016 CEE Spring articles - || ' + str(total) + ' || ' + (u'▒' * rep) + '\n'
    t += u'|}\n'

    t += u'=== New articles about countries ===\n'
    t += u'{|\n'
    s2max = max(stats_by_country.values())
    point = 1.0 * max_line / s2max
    if point > 1:
        point = 1
    # for co in sorted(stats_by_country):
    for co in sorted(stats_by_country.keys(), lambda x, y: stats_by_country[y] - stats_by_country[x]):
        rep = int(round(point * stats_by_country[co]))
        if stats_by_country[co] > 0 and rep == 0:
            rep = 1
        t += u'|-\n'

        if debug:
            count_str = str(stats_by_country[co]) + ' - [[User:Botik/Stats/'+co+'|details]]'
        else:
            count_str = str(stats_by_country[co]) + ' - [[Wikimedia_CEE_Spring_2016/Structure/Statistics/'+co+'|details]]'

        t += u'| ' + co + ' - || '+ count_str +' || ' + (u'▒' * rep) + '\n'
        #t += u'| ' + co + ' || ' + str(stats_by_country[co]) + ' || ' + (u'▒' * rep) + '\n'
    t += u'|}\n'

    t += u'=== New articles by languages ===\n'
    t += u'{|\n'
    s3max = max(stats_by_lang.values())
    point = 1.0 * max_line / s3max
    if point > 1:
        point = 1
    # for la in sorted(stats_by_lang):
    for la in sorted(stats_by_lang.keys(), lambda x, y: stats_by_lang[y] - stats_by_lang[x]):
        rep = int(round(point * stats_by_lang[la]))
        if stats_by_lang[la] > 0 and rep == 0:
            rep = 1
        t += u'|-\n'
        t += u'| ' + '{{H:title|' + lang_names[la] + u'|' + la.replace(u'be-tarask', u'be-t') + u'}}' + ' || ' + str(
            stats_by_lang[la]) + ' || ' + (u'▒' * rep) + '\n'
    t += u'|}\n'

    t += u'=== Last 500 articles ===\n'
    for i, date in enumerate(sorted(stats_by_date, reverse=True)):
        itm = date.strftime("%Y-%m-%d %H:%M:%S")
        itm += ' [[:w:'+stats_by_date[date]+']]'
        t += u'* '+ itm+'\n'
        if i >= 500:
            break

    if not debug:
        savePage('Wikimedia_CEE_Spring_2016/Structure/Statistics', t)
    else:
        savePage('User:Botik/Stats', t)


def get_country_qs(country):
    wiki_text = ''
    try:
        co_list_req = urllib2.Request(
            "http://meta.wikimedia.org/w/api.php?format=json&action=query&prop=revisions&rvprop=content&pageids="+get_subpage_ids(country)+"&continue=")
        co_list_resp = urllib2.build_opener().open(co_list_req).read()
        co_list_json = json.loads(co_list_resp)
        # print "http://meta.wikimedia.org/w/api.php?format=json&action=query&prop=revisions&rvprop=content&titles=Wikimedia_CEE_Spring_2015/Structure/"+country+"&continue="

        for itm in co_list_json["query"]["pages"]:
            wiki_text += co_list_json["query"]["pages"][itm]["revisions"][0]["*"] + "\n"
    except Exception as e:
        print("".join(traceback.format_exception(sys.exc_info()[0], sys.exc_info()[1], sys.exc_info()[2])))

    qs = list()
    topics = dict()
    mat = ''
    topic = ''
    for line in wiki_text.split('\n'):
        # <h3 style="color:#339966;">[[File:P biology-green.png|30px]] '''b. Nature / Geography'''</h3>
        # === [[File:P agriculture-green.png|30px]] '''Economics''' ===
        # === Culture ===
        match = re.search(ur'(==(.*)==|<h3.*>(.*)</h3>)', line, re.IGNORECASE)
        if match is not None:
            if match.group(2) is not None:
                mat = match.group(2).strip()
            else:
                mat = match.group(3).strip()
            mat = mat.replace("'''", "").strip()
            mat = mat.replace("=", "").strip()
            mat = re.sub('\[\[.*\]\]', '', mat).strip()
            mat = re.sub('[a-z]\. ', '', mat).strip()

            topic = mat

        # {{#invoke:WikimediaCEETable|table|Q948201|Q834689}}
        match = re.search(ur'\{\{#invoke\:WikimediaCEETable\|table\|(.*)\}\}', line, re.IGNORECASE)
        if match is not None:
            mat = match.group(1).strip()
            qs += mat.split('|')

            for q_el in mat.split('|'):
                topics[q_el] = topic

        elif:
            # [[d:Q2499614|Q2499614]]
            match = re.search(ur"\[\[\:?d\:Q(\d+?)\|Q(\d+?)\]\]", line, re.IGNORECASE)
            if match is not None:
                mat = match.group(1).strip()
                qs.append('Q' + q)
                topics['Q' + q] = topic


    return qs, topics


def get_subpage_ids(country):
    # https://meta.wikimedia.org/wiki/Wikimedia_CEE_Spring_2016/Structure/Bulgaria
    # https://meta.wikimedia.org/wiki/Wikimedia_CEE_Spring_2016/Structure/Bulgaria/Science

    subpage_list=[]
    req = urllib2.Request(
        'https://meta.wikimedia.org/w/api.php?format=json&action=query&list=allpages&aplimit=max&apprefix=Wikimedia+CEE+Spring+2016%2FStructure%2F' + urllib2.quote(
            country.encode('utf-8')) )
    resp = urllib2.build_opener().open(req).read()
    r_json = json.loads(resp)

    for itm in r_json["query"]["allpages"]:
        subpage_list.append(str(itm["pageid"]))

    return '|'.join(subpage_list)


def get_county_list():
    county_list = []
    wiki_text = ''

    req = urllib2.Request(
        'http://meta.wikimedia.org/w/api.php?format=json&action=query&prop=revisions&rvprop=content&titles=' +
        urllib2.quote('Template:Wikimedia CEE Spring 2016 Article Lists by Country'.encode('utf-8')) + '&continue=')
    resp = urllib2.build_opener().open(req).read()
    r_json = json.loads(resp)

    for itm in r_json["query"]["pages"]:
        wiki_text = r_json["query"]["pages"][itm]["revisions"][0]["*"]


    mat = ''
    for line in wiki_text.split('\n'):
        # File:Flag_of_Albania.svg|<br />[[Wikimedia_CEE_Spring_2016/Structure/Albania|{{Label|Q222}}]]|link=[[Wikimedia_CEE_Spring_2016/Structure/Albania]]|alt=Albania
        match = re.search(ur'\[\[Wikimedia.CEE.Spring.2016/Structure/([^\|]*?)\]\]', line, re.IGNORECASE)
        if match is not None:
            mat = match.group(1).strip()
            county_list += mat.split('|')

    return county_list


def get_lang_list():
    lang_list = []

    q='Q22342981'
    r_url = "http://www.wikidata.org/w/api.php?format=json&action=wbgetentities&ids="+q+"&props=sitelinks"

    item_req = urllib2.Request(r_url)
    item_resp = urllib2.build_opener().open(item_req).read()
    item_json = json.loads(item_resp)

    item_link_dict = {}

    for item_link in item_json["entities"][q]["sitelinks"]:
        item_link_lang = item_json["entities"][q]["sitelinks"][item_link]["site"]
        item_link_lang = item_link_lang.replace('wiki', '')
        item_link_lang = item_link_lang.replace('be_x_old', 'be-tarask')
        if item_link_lang!='meta':
            lang_list.append(item_link_lang)

    return sorted(lang_list)

def save_country_table(country):
    txt = u''
    txt = u"{{Notice|This is bot generated statistics about new and edited articles from proposal list of the country. It's updated nightly.}}" + '\n'
    txt = txt + u'{{Notice|Auto generated by [[user:Botik]] at ' + strftime("%Y-%m-%d %H:%M:%S", gmtime()) + ' UTC}}' + '\n'
    txt = txt + '''
{| class="wikitable"
|-
! Color !! Meaning
|-
| style="background:#98FB98" | green || new articles
|-
| style="background:#ffc757" | orange || edited articles
|-
| style="background:#FFFF00" | yellow || old unchanged articles
|}
'''
    txt = txt + u'=== ' + country + '===\n'
    txt = txt + u'{| class="wikitable"' + '\n'
    txt = txt + u'|-' + '\n'

    txt = txt + u'! style="width:20em" | {{H:title|English article|en}}' + '\n'
    txt = txt + u'! style="width:6em" | wikidata' + '\n'
    for key in sorted(langs):
        txt = txt + u'! {{H:title|' + lang_names[key] + u'|' + key.replace(u'be-tarask', u'be-t') + u'}}' + u'\n'

    for topic in sorted(big_country_table[country]):
        txt = txt + u'|-' + '\n'
        txt = txt + u'| colspan="' + str(len(langs) + 2) + '" style="background:#dddddd" | ' + topic + '\n'

        for label in sorted(big_country_table[country][topic]):
            for q in big_country_table[country][topic][label]:
                txt = txt + u'|-' + '\n'
                txt = txt + u'| ' + label + '\n'
                txt = txt + u'| style="background:#fff8dc"| ' + u'[[:d:' + q + u'|' + q + u']]' + '\n'

                for t_lang in sorted(langs):
                    if big_country_table[country][topic][label][q][t_lang][0]:
                        colors = {'new': '#98FB98', 'edited': '#ffc757', 'old': '#FFFF00'}
                        color_val = big_country_table[country][topic][label][q][t_lang][0]
                        link = big_country_table[country][topic][label][q][t_lang][1]
                        txt = txt + u'| style="background:'+colors[color_val]+'"|[[:' + t_lang + u':' + link + u'|+]]' + '\n'
                    else:
                        txt = txt + u'| ' + '\n'

    txt=txt+ '|}'+'\n'

    if not debug:
        savePage('Wikimedia_CEE_Spring_2016/Structure/Statistics/'+country, txt)
    else:
        savePage('User:Botik/Stats/'+country, txt)

try:
    f = open('./cee_cache.txt', 'r')
    cache = json.load(f)
    f.close()
except:
    cache = {}
    print('cache load error')
    print("".join(traceback.format_exception(sys.exc_info()[0], sys.exc_info()[1], sys.exc_info()[2])))

try:
    f = open('./cee_cache_le.txt', 'r')
    cache_le = json.load(f)
    f.close()
    for key, value in cache_le.iteritems():
        cache_le[key] = datetime.strptime(value, "%Y-%m-%dT%H:%M:%S")
except:
    cache_le = {}
    print('cache_le load error')
    print("".join(traceback.format_exception(sys.exc_info()[0], sys.exc_info()[1], sys.exc_info()[2])))

UTF8Writer = codecs.getwriter('utf8')
sys.stdout = UTF8Writer(sys.stdout)



lang_names = {'sh': 'Serbo-Croatian - srpskohrvatski jezik', 'eo': u'Esperanto', 'az': u'Azerbaijani - azərbaycan dili',
              'ba': u'Bashkir - башҡорт теле', 'be': u'Belarusian - беларуская мова',
              'be-tarask': u'Belarusian (Taraškievica) - беларуская мова (тарашкевіца)',
              'bg': u'Bulgarian - български език', 'bs': u'Bosnian - bosanski jezik', 'ce': u'Chechen - нохчийн мотт',
              'crh': u'Crimean Tatar - Къырым Татар', 'cs': u'Czech - čeština', 'cv': u'Chuvash - чӑваш чӗлхи',
              'de': 'German - Deutsche', 'el': u'Greek - ελληνικά', 'et': u'Estonian - eesti',
              'hr': u'Croatian - hrvatski jezik', 'hu': u'Hungarian - magyar', 'hy': u'Armenian - Հայերեն',
              'ka': u'Georgian - ქართული', 'kk': u'Kazakh - қазақ тілі', 'lt': u'Lithuanian - lietuvių kalba',
              'lv': u'Latvian - latviešu valoda', 'mk': u'Macedonian - македонски јазик',
              'os': u'Ossetian - ирон æвзаг', 'pl': u'Polish - polszczyzna', 'ro': u'Romanian - limba română',
              'ru': u'Russian - русский язык', 'rue': u'Rusyn - русин', 'sk': u'Slovak - slovenčina',
              'sl': u'Slovene - slovenščina', 'sq': u'Albanian - Shqip', 'sr': u'Serbian - српски језик',
              'tr': u'Turkish - Türkçe', 'tt': u'Tatar - татар теле', 'uk': u'Ukrainian - українська мова',
              'sah': u'Sakha - саха тыла'}

debug = False


if debug:
    langs = ['ru', 'pl', 'uk', 'be']
    langs = sorted(get_lang_list())
    countries = [u'Serbia']
else:
    langs = sorted(get_lang_list())
    countries = sorted(get_county_list())

print 'Countries: '+str(len(countries))
print countries
print 'Languages: '+str(len(langs))
print langs

for la in langs:
    if la not in lang_names:
        lang_names[la]=la

# once a day full processing
if (datetime.now().hour in [0,1]) or (debug) or ('--full' in sys.argv):
    print 'Full processing'
    full_processing = True
else:
    full_processing = False

stats_orig_list = {}
stats_by_country = {}
stats_by_lang = {}
stats_by_date = {}

for country in sorted(countries):
    stats_orig_list[country] = 0
    stats_by_country[country] = 0
for lang in langs:
    stats_by_lang[lang] = 0

for country in sorted(countries):
    dd = lambda: defaultdict(dd)
    big_country_table = dd()

    # big_table['Bulgaria']['Culture']['Q123']['Sofia theatre 2']['ru'] = 'n'

    q_list = list()
    country_items = list()

    print '\n+++++++++++' + country
    qs, topics = get_country_qs(country)
    print '+++++++++++' + str(len(qs))

    #if debug:
    #    qs = qs[0:5]

    for q in qs:
        print q

        r_url = "http://www.wikidata.org/w/api.php?format=json&action=wbgetentities&ids=" + q + "&props=labels|sitelinks"

        item_req = urllib2.Request(r_url)
        item_resp = urllib2.build_opener().open(item_req).read()
        item_json = json.loads(item_resp)

        item_label_dict = {}
        item_link_dict = {}

        try:

            for la in langs:
                item_label_dict[la] = ''
                item_link_dict[la] = ''

            for q2 in item_json["entities"]:
                if q2 == '-1':
                    continue
                q = q2

            if q in q_list:
                continue
            elif q != '':
                q_list.append(q)

            for item_label in item_json["entities"][q]["labels"]:
                item_label_lang = item_json["entities"][q]["labels"][item_label]["language"]
                item_label_value = item_json["entities"][q]["labels"][item_label]["value"]

                if item_label_lang in langs or item_label_lang == 'en':
                    item_label_dict[item_label_lang] = item_label_value

            if "sitelinks" in item_json["entities"][q]:
                for item_link in item_json["entities"][q]["sitelinks"]:
                    item_link_lang = item_json["entities"][q]["sitelinks"][item_link]["site"]
                    item_link_lang = item_link_lang.replace('wiki', '')
                    item_link_lang = item_link_lang.replace('be_x_old', 'be-tarask')
                    item_link_value = item_json["entities"][q]["sitelinks"][item_link]["title"]

                    if item_link_lang in langs or item_link_lang == 'en':
                        item_link_dict[item_link_lang] = item_link_value

            stats_orig_list[country] += 1

            if 'en' in item_label_dict:
                item_main_label = item_label_dict['en']
            else:
                # first nonempty value, latin script first
                item_main_label = sorted([i for i in item_label_dict.values() if len(i)>1 ])[0]

            for key in sorted(item_link_dict):
                if (key != 'en') and (len(item_link_dict[key]) > 1):
                    first_edit = getFirstEdit(key, item_link_dict[key])

                    # new articles
                    if (first_edit >= datetime.strptime('2016-03-21', "%Y-%m-%d")) and (first_edit <= datetime.strptime('2016-05-29', "%Y-%m-%d")):
                        stats_by_country[country] += 1
                        stats_by_lang[key] += 1
                        stats_by_date[first_edit] = key + ':' + item_link_dict[key]
                        big_country_table[country][topics[q]][item_main_label][q][key] = ['new', item_link_dict[key]]

                    # old articles
                    elif (first_edit < datetime.strptime('2016-03-21', "%Y-%m-%d")) and full_processing:

                        if key + ':' + item_link_dict[key] in cache_le:
                            last_edit = cache_le[key + ':' + item_link_dict[key]]
                        else:
                            last_edit = getLastEdit(key, item_link_dict[key])

                        if (last_edit >= datetime.strptime('2016-03-21', "%Y-%m-%d")) and (last_edit <= datetime.strptime('2016-05-29', "%Y-%m-%d")):
                            # changed old article (orange)

                            # запоминаем, что статья уже менялась после начала конкурса
                            cache_le[key + ':' + item_link_dict[key]] = last_edit
                            big_country_table[country][topics[q]][item_main_label][q][key] = ['edited', item_link_dict[key]]

                        elif last_edit < datetime.strptime('2016-03-21', "%Y-%m-%d"):
                            # unchanged old article (yellow)
                            big_country_table[country][topics[q]][item_main_label][q][key] = ['old', item_link_dict[key]]



        except Exception as e:
            print("".join(traceback.format_exception(sys.exc_info()[0], sys.exc_info()[1], sys.exc_info()[2])))

    if full_processing:
        save_country_table(country)

print " "

'''
for tab_country in big_country_table:
    for tab_topic in big_country_table[tab_country]:
        for tab_label in big_country_table[tab_country][tab_topic]:
            for tab_q in big_country_table[tab_country][tab_topic][tab_label]:
                for tab_lang in big_country_table[tab_country][tab_topic][tab_label][tab_q]:
                    val = big_country_table[tab_country][tab_topic][tab_label][tab_q][tab_lang][0]
                    link = big_country_table[tab_country][tab_topic][tab_label][tab_q][tab_lang][1]
                    if not val:
                        val = u'NA'
                    if not link:
                        link = u'NA'
                    print tab_country+' - '+tab_topic+' - '+tab_q+' - '+tab_label+' - '+tab_lang+' - '+val+' - '+link
'''

PublishStats()

try:
    f = open('./cee_cache.txt', 'w')
    json.dump(cache, f)
    f.close()
except:
    print('Failed to save cache')
    print("".join(traceback.format_exception(sys.exc_info()[0], sys.exc_info()[1], sys.exc_info()[2])))

json.JSONEncoder.default = lambda self, obj: (obj.isoformat() if isinstance(obj, datetime) else None)
try:
    f2 = open('./cee_cache_le.txt', 'w')
    json.dump(cache_le, f2)
    f2.close()
except:
    print('Failed to save cache_le')
    print("".join(traceback.format_exception(sys.exc_info()[0], sys.exc_info()[1], sys.exc_info()[2])))
